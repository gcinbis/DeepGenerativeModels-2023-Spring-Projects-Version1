MaskGIT: Masked Generative Image Transformer
Link: https://arxiv.org/pdf/2202.04200.pdf


Quantitative result: Table 1, ImageNet256x256 part, given FID of MaskGIT (scaled, see below for more information)
Qualitative result: Figure 7, top row

The benchmarks given are evaluated only on the large ImageNet dataset, which demands significant computational power for training. To mitigate this, we plan to train the model on a subset of the dataset. To fairly evaluate the performance, a scaling factor can be used to adjust the evaluation metrics. This factor can be empirically derived by training models on multiple subsamples and estimating the dataset size-performance relationship. The evaluation metrics are then adjusted using this scaling factor.

---------------------------------------------------------------------------------

The quantitative result goals are updated from class-conditional image synthesis to image inpainting in Table 2, as per the clearer instructions on the implementation. Additionally, we are using ImageNet64 dataset, as per the memory constraints.
The results are not complete yet, as the model is still being trained, but the loss curves obtained seem promising.