{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Information\n",
    "Name: GANSeg: Learning to Segment by Unsupervised Hierarchical Image Generation\n",
    "Link: https://arxiv.org/pdf/2112.01036.pdf\n",
    "    \n",
    "# Authors of Code\n",
    "Ahmet Kağan Kaya - 2598555 - \"kagan.kaya@metu.edu.tr\"\n",
    "\n",
    "# Paper Summary\n",
    "This paper proposes a GAN-based approach that generates images conditioned on latent masks, thereby alleviating full or weak annotations required by previous approaches. Part and background appearances are controlled by latent space and image generation is done by using both Point and Background Generator. Without requiring supervision of masks or points, this strategy increases robustness of mask to viewpoint and object position changes. It also lets us generate image-mask pairs for training a segmentation network, which outperforms state-of-the-art unsupervised segmentation methods on established benchmark.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"archi.png\" width=\"700\"/>\n",
    "<figcaption>Overall Architecture with 3 Different Levels</figcaption>\n",
    "</div>\n",
    "\n",
    "# Method\n",
    "## Level 1: Point Generation and Part Scale\n",
    "In the first level, independent noise vectors are used to generate the locations and appearances of $K$ parts. It is found that training can be stabilized by first predicting $n_{per} × K$ points. The part location and scale are computed from the mean and standard deviation of the corresponding $n_{per}$ points, which also regularizes training.\n",
    "\n",
    "$$ x_k = \\frac{1}{n_\\text{per}}\\sum_{i=1}^{n_\\text{per}} x_k^i, \\quad \\sigma_k = \\frac{\\sqrt{\\sum_i^{n_\\text{per}}\\|x_k^i- x_k\\|^2}}{n_\\text{per}-1}$$ \n",
    "$$ \\text{with }\\{x_k^{1}, ...,x_k^{n_\\text{per}}\\}_{k=1}^K = \\text{MLP}_\\text{point}(z_\\text{point}) $$ \n",
    "\n",
    "Again 3-layer multi-layer perceptron (MLP) is used to map $z_\\text{point}$ to $n_\\text{per}\\times K$ points $\\{x_k^{1}, ...,x_k^{n_\\text{per}}\\}_{k=1}^K$. \n",
    "Then part locations are calculated in terms of $\\{x_1,...,x_K\\}$ and part scales $\\{\\sigma_1,...,\\sigma_K\\}$\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"level1.png\" width=\"700\"/>\n",
    "<figcaption>Strategy for Level 1</figcaption>\n",
    "</div>\n",
    "\n",
    "## Level 2: From Points to Masks\n",
    "In the second level, Gaussian heatmaps are use to create masks for each image and model is used to generate the local independence and positional encoding  masks relative to the predicted part location. Gaussian heatmaps are generated for each part using the mean and standard deviation of each part defined in Level 1. The embedding $w_k$ is then multiplied with every pixel of the corresponding heatmap, generating a spatially localized embedding map. All $K$ part-specific embeddings are summed to form a single feature map $W_\\text{mask}\\in\\R^{D_\\text{emb}\\times H\\times W}$.\n",
    "$$ H_k(p)=\\exp\\left(-\\|p- x_k\\|_2^2 / \\sigma_k^2\\right) $$\n",
    "$$ W_\\text{mask}(p) = \\sum_{k=1}^K H_k(p)w_k. $$\n",
    "\n",
    "Generated embedding map $W_\\text{mask}$ will subsequently be used to generate masks, together with the mask starting tensor $M^{(0)}\\in\\R^{D_\\text{emb}\\times H\\times W}$. HOwever, instead of using constant tensor, it is better to use low frequenct positional embedding:\n",
    "\n",
    "$$ M^{(0)}(p) = [\\sin(\\pi\\text{FC}([p-x^1_1, ..., p-x^{n_\\text{per}}_K])), \n",
    "        \\cos(\\pi\\text{FC}([p-x^1_1, ..., p-x^{n_\\text{per}}_K]))] $$\n",
    "\n",
    "After that all obtain results are put on the SPADE ResBlock which is proposed in this paper. SPADE takes two feature maps as input. First use BatchNorm to\n",
    "normalize input followed by two convolutions to map to the new mean and new standard deviation.\n",
    "\n",
    "$$ M^{(i)} = \\text{SPADE ResBlock} (M^{(i-1)}, W_\\text{mask})\\\\\n",
    "        M = \\text{softmax}(M^{(T_\\text{mask})}) $$\n",
    "        \n",
    "<div align=\"center\">\n",
    "<img src=\"mask_gen.png\" width=\"700\"/>\n",
    "<figcaption>Strategy for Level 2</figcaption>\n",
    "</div>\n",
    "\n",
    "## Level 3: Mask-conditioned Image Generation\n",
    "In this level, foreground and the background are generated separately and blend them linearly by reusing the masks from the previous level. Embedding maps of both foreground and background are generated seperately:\n",
    "\n",
    "$$ W_\\text{fg}(p) = \\sum_{k=1}^K M_k(p) w_k. $$\n",
    "$$ W_\\text{bg} = \\text{MLP}_\\text{bg\\_app}(z_\\text{bg\\_app}). $$\n",
    "\n",
    "These wieghts are used to generate background and foreground:\n",
    "$$ F^{(i)} =  \\text{SPADE ResBlock} (F^{(i-1)}, W_\\text{fg}) $$\n",
    "$$ B^{(i)} = \\text{AdaIN ConvBlock}  (B^{(i-1)}, W_\\text{bg}) $$\n",
    "\n",
    "All obtained results are concataned to get final mask and result:\n",
    "$$ I = \\text{Conv}((1-M_\\text{bg})\\otimes F + M_\\text{bg}\\otimes B) $$\n",
    "\n",
    "<p>\n",
    "  <img src=\"foreground.png\" width=\"700\" />\n",
    "  <img src=\"background.png\" width=\"700\" /> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import importlib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dataset import CelebAWildTrain\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import os\n",
    "from model import Generator\n",
    "from other_models import Discriminator\n",
    "from tqdm import tqdm\n",
    "from evaluate import evaluate\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "All parameters are stored in object of Params:\n",
    "\n",
    "All important hyperparemeters such as learning rates are obtained from paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 256\n",
    "        self.cluster_number = 8\n",
    "        self.n_per_kp = 4\n",
    "        self.batch_size = 16\n",
    "        self.lr_gen = 1e-4 \n",
    "        self.lr_disc = 4e-4 \n",
    "        self.num_workers = 0\n",
    "        self.data_root = ''\n",
    "        self.class_name = 'celeba_wild'\n",
    "        self.image_size = 128\n",
    "        self.embedding_dim = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Params()\n",
    "args.log = \"GanSEG\"\n",
    "\n",
    "os.makedirs(args.log, exist_ok=True)\n",
    "with open(os.path.join(args.log, 'parameters.json'), 'wt') as f:\n",
    "    json.dump(args.__dict__, f, indent=2)\n",
    "\n",
    "device = 'cuda:0'\n",
    "device = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "generator = Generator(args).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "optim_disc = torch.optim.Adam(discriminator.parameters(), lr=args.lr_disc, betas=(0.5, 0.9))\n",
    "optim_gen = torch.optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=args.lr_gen, betas=(0.5, 0.9))\n",
    "\n",
    "generator = torch.nn.DataParallel(generator)\n",
    "discriminator = torch.nn.DataParallel(discriminator)\n",
    "\n",
    "checkpoint_dir = os.path.join(args.log, 'checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Architecture\n",
      "DataParallel(\n",
      "  (module): Generator(\n",
      "    (keypoints_embedding): Embedding(8, 128)\n",
      "    (mask_spade_blocks): ModuleList(\n",
      "      (0): SPADE(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv6): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (1): SPADE(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (2): SPADE(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (3): SPADE(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv6): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (spade_blocks): ModuleList(\n",
      "      (0): SPADE(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (1): SPADE(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv6): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (2): SPADE(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (3): SPADE(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (adain_blocks): ModuleList(\n",
      "      (0): AdaIN(\n",
      "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv_std): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (conv_mean): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (1): AdaIN(\n",
      "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv_std): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (conv_mean): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (2): AdaIN(\n",
      "        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv_std): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv_mean): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (3): AdaIN(\n",
      "        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv_std): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv_mean): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (x_start): PositionalEmbedding(\n",
      "      (pe): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (mask_start): PositionalEmbedding(\n",
      "      (pe): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (bg_start): PositionalEmbedding(\n",
      "      (pe): Conv2d(2, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (rep_pad): ReplicationPad2d((10, 10, 10, 10))\n",
      "    (gen_keypoints_embedding_noise): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.2)\n",
      "      (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (gen_keypoints_layer): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.2)\n",
      "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (7): LeakyReLU(negative_slope=0.2)\n",
      "      (8): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "    (gen_background_embedding): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (5): LeakyReLU(negative_slope=0.2)\n",
      "      (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (2): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Discriminator Architecture\n",
      "DataParallel(\n",
      "  (module): Discriminator(\n",
      "    (net): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (16): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=32768, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Generator Architecture\")\n",
    "print(generator)\n",
    "print(\"Discriminator Architecture\")\n",
    "print(discriminator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CelebAWildTrain(args.data_root, args.image_size)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, pin_memory=True, drop_last=True)\n",
    "test_input_batch = {'input_noise{}'.format(noise_i): torch.randn(args.batch_size, *noise_shape).to(device)\n",
    "                    for noise_i, noise_shape in enumerate(generator.module.noise_shapes)}\n",
    "test_input_batch['bg_trans'] = torch.rand(args.batch_size, 1, 2).to(device) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45609, 3, 128, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(500):    \n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "    total_disc_loss = 0\n",
    "    total_gen_loss = 0\n",
    "    prog = tqdm(enumerate(data_loader))\n",
    "    for batch_index, batch in prog:\n",
    "        # update discriminator\n",
    "        optim_disc.zero_grad()\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        batch = {'img': batch['img'].to(device)}\n",
    "        batch['img'].requires_grad_()\n",
    "        input_gen = {'input_noise{}'.format(noise_i): torch.randn(args.batch_size, *noise_shape).to(device)\n",
    "                    for noise_i, noise_shape in enumerate(generator.module.noise_shapes)}\n",
    "        input_gen['bg_trans'] = torch.rand(args.batch_size, 1, 2).to(device) * 2 - 1\n",
    "\n",
    "        fake_input = generator(input_gen)\n",
    "        real_disc = discriminator(batch)\n",
    "        fake_disc = discriminator(fake_input)\n",
    "        disc_loss = F.softplus(fake_disc).mean() + F.softplus(-real_disc).mean()\n",
    "        if batch_index % 4 == 0:\n",
    "            disc_loss = disc_loss + penalty(batch['img'], real_disc)\n",
    "        disc_loss.backward()\n",
    "        total_disc_loss += disc_loss.item()\n",
    "        optim_disc.step()\n",
    "\n",
    "        # update generator\n",
    "        optim_disc.zero_grad()\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        input_gen = {'input_noise{}'.format(noise_i): torch.randn(args.batch_size, *noise_shape).to(device)\n",
    "                        for noise_i, noise_shape in enumerate(generator.module.noise_shapes)}\n",
    "        input_gen['bg_trans'] = torch.rand(args.batch_size, 1, 2).to(device) * 2 - 1\n",
    "        fake_input = generator(input_gen)\n",
    "        fake_disc = discriminator(fake_input)\n",
    "        gen_loss = F.softplus(-fake_disc).mean()\n",
    "        gen_loss.backward()\n",
    "        total_gen_loss += gen_loss.item()\n",
    "        optim_gen.step()\n",
    "        disc_loss, gen_loss = total_disc_loss / len(data_loader) / 2, total_gen_loss / len(data_loader)\n",
    "        if batch_index == 1000:\n",
    "            break\n",
    "        prog.set_description(f\"Epoch: {epoch + 1}, disc_loss:  {disc_loss}, gen_loss: {gen_loss}\")\n",
    "    evaluate(generator, test_input_gen, args, epoch)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        torch.save({'generator': generator.module.state_dict(), 'discriminator': discriminator.module.state_dict(), 'optim_gen': optim_gen.state_dict(), 'optim_disc': optim_disc.state_dict(),},os.path.join(checkpoint_dir, 'epoch_{}.model'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, disc_loss:  0.91602554521552, gen_loss: 2.9154146088120627: : 1000it [17:04,  1.02s/it]  \n",
      "Epoch: 2, disc_loss:  0.19575286904447958, gen_loss: 1.3049921820260453: : 1000it [17:09,  1.03s/it]  \n",
      "Epoch: 3, disc_loss:  0.16392073070793822, gen_loss: 1.1133889933113466: : 1000it [17:23,  1.04s/it]  \n",
      "Epoch: 4, disc_loss:  0.1673950523660894, gen_loss: 1.0478422773302647: : 1000it [17:18,  1.04s/it]    \n",
      "Epoch: 5, disc_loss:  0.16378850753892932, gen_loss: 0.906951866683207: : 1000it [17:26,  1.05s/it]   \n",
      "Epoch: 6, disc_loss:  0.1642864903337077, gen_loss: 0.8528890935204139: : 1000it [17:25,  1.05s/it]   \n",
      "Epoch: 7, disc_loss:  0.15886002495100623, gen_loss: 0.7847563005016561: : 1000it [17:24,  1.04s/it]   \n",
      "Epoch: 8, disc_loss:  0.16731682119139454, gen_loss: 0.7638920359601055: : 1000it [17:26,  1.05s/it]   \n",
      "Epoch: 9, disc_loss:  0.1768046492233611, gen_loss: 0.7328230114464174: : 1000it [17:24,  1.04s/it]    \n",
      "Epoch: 10, disc_loss:  0.18231592197167246, gen_loss: 0.7279043149006994: : 1000it [17:23,  1.04s/it]   \n",
      "Epoch: 11, disc_loss:  0.246766946399421, gen_loss: 0.7160550303835618: : 1000it [17:22,  1.04s/it]     \n",
      "Epoch: 12, disc_loss:  0.20105214021707837, gen_loss: 0.6891262086337073: : 1000it [17:27,  1.05s/it]  \n",
      "Epoch: 13, disc_loss:  0.5057592329892673, gen_loss: 0.6789854133338259: : 1000it [17:26,  1.05s/it]    \n",
      "Epoch: 14, disc_loss:  0.21817433720617962, gen_loss: 0.6659025987721326: : 1000it [17:27,  1.05s/it]   \n",
      "Epoch: 15, disc_loss:  0.2704837605278743, gen_loss: 0.6581666157329291: : 1000it [17:30,  1.05s/it]    \n",
      "Epoch: 16, disc_loss:  0.2122004973470119, gen_loss: 0.6538607457541583: : 1000it [17:30,  1.05s/it]    \n",
      "Epoch: 17, disc_loss:  0.19493646167872244, gen_loss: 0.6362794649182705: : 1000it [17:27,  1.05s/it]   \n",
      "Epoch: 18, disc_loss:  0.24378937245983825, gen_loss: 0.6510043276506557: : 1000it [17:29,  1.05s/it]  \n",
      "Epoch: 19, disc_loss:  0.19404490737015742, gen_loss: 0.6428587244581758: : 1000it [17:36,  1.06s/it]   \n",
      "Epoch: 20, disc_loss:  0.18144089603633212, gen_loss: 0.6332077617080588: : 1000it [17:30,  1.05s/it]   \n",
      "Epoch: 21, disc_loss:  0.20403113204659076, gen_loss: 0.6409609125266995: : 1000it [17:28,  1.05s/it]  \n",
      "Epoch: 22, disc_loss:  0.20163132625190835, gen_loss: 0.645880325708473: : 1000it [17:29,  1.05s/it]    \n",
      "Epoch: 23, disc_loss:  0.19517553195618748, gen_loss: 0.6479485378662745: : 1000it [17:31,  1.05s/it]  \n",
      "Epoch: 24, disc_loss:  0.1884628846247991, gen_loss: 0.6344751320805466: : 1000it [17:32,  1.05s/it]    \n",
      "Epoch: 25, disc_loss:  0.18489027134682004, gen_loss: 0.6284861286219797: : 1000it [17:28,  1.05s/it]   \n",
      "Epoch: 26, disc_loss:  0.18329135458720358, gen_loss: 0.6338490488184125: : 1000it [17:27,  1.05s/it]    \n",
      "Epoch: 27, disc_loss:  0.17737608559298934, gen_loss: 0.6358133276931026: : 1000it [17:30,  1.05s/it]  \n",
      "Epoch: 28, disc_loss:  0.17247175581099694, gen_loss: 0.6248141477819075: : 1000it [17:32,  1.05s/it]   \n",
      "Epoch: 29, disc_loss:  0.18309974986210203, gen_loss: 0.6265653532063752: : 1000it [17:32,  1.05s/it]   \n",
      "Epoch: 30, disc_loss:  0.19481142593057532, gen_loss: 0.6276236511845338: : 1000it [17:33,  1.05s/it]   \n",
      "Epoch: 31, disc_loss:  0.18011580981706318, gen_loss: 0.6169128438804233: : 1000it [17:33,  1.05s/it]   \n",
      "Epoch: 32, disc_loss:  0.1887586452459034, gen_loss: 0.6249460739106463: : 1000it [17:29,  1.05s/it]   \n",
      "Epoch: 33, disc_loss:  0.17971346737522828, gen_loss: 0.6218632769689225: : 1000it [17:31,  1.05s/it]   \n",
      "Epoch: 34, disc_loss:  0.19088598083508643, gen_loss: 0.6408257004984638: : 1000it [17:29,  1.05s/it]   \n",
      "Epoch: 35, disc_loss:  0.1721425866976119, gen_loss: 0.6195369961596372: : 1000it [17:27,  1.05s/it]    \n",
      "Epoch: 36, disc_loss:  0.18127838198030205, gen_loss: 0.6163180573519907: : 1000it [17:30,  1.05s/it]   \n",
      "Epoch: 37, disc_loss:  0.19388023901403997, gen_loss: 0.621880965384475: : 1000it [17:29,  1.05s/it]    \n",
      "Epoch: 38, disc_loss:  0.17794316802108498, gen_loss: 0.6253662479446646: : 1000it [17:25,  1.05s/it]   \n",
      "Epoch: 39, disc_loss:  0.18326451484048575, gen_loss: 0.6142191594316249: : 1000it [17:27,  1.05s/it]  \n",
      "Epoch: 40, disc_loss:  0.22463075828656814, gen_loss: 0.6140654158932075: : 1000it [17:28,  1.05s/it]   \n",
      "Epoch: 41, disc_loss:  0.20923719453707076, gen_loss: 0.6165765097737312: : 1000it [17:32,  1.05s/it]   \n",
      "Epoch: 42, disc_loss:  0.2754581312390796, gen_loss: 0.6248016891772287: : 1000it [17:28,  1.05s/it]    \n",
      "Epoch: 43, disc_loss:  0.21539401654088705, gen_loss: 0.6123160480720955: : 1000it [17:26,  1.05s/it]  \n",
      "Epoch: 44, disc_loss:  0.19884948955293288, gen_loss: 0.6045724131245362: : 1000it [17:27,  1.05s/it]  \n",
      "Epoch: 45, disc_loss:  0.19975812158040834, gen_loss: 0.594257944205351: : 1000it [17:25,  1.05s/it]   \n",
      "Epoch: 46, disc_loss:  0.18140019585166062, gen_loss: 0.6024304033162301: : 1000it [17:29,  1.05s/it]   \n",
      "Epoch: 47, disc_loss:  0.20090101981372163, gen_loss: 0.6058241386685455: : 1000it [17:23,  1.04s/it]   \n",
      "Epoch: 48, disc_loss:  0.18631611475296186, gen_loss: 0.6001833874091768: : 1000it [17:24,  1.04s/it]   \n",
      "Epoch: 49, disc_loss:  0.19927488619821113, gen_loss: 0.6048985071271135: : 1000it [17:24,  1.04s/it]   \n",
      "Epoch: 50, disc_loss:  0.18598626392975187, gen_loss: 0.5848008295004828: : 1000it [17:22,  1.04s/it]   \n",
      "Epoch: 51, disc_loss:  0.20315842558417405, gen_loss: 0.5906121135803691: : 1000it [17:23,  1.04s/it]   \n",
      "Epoch: 52, disc_loss:  0.188537053978234, gen_loss: 0.5852501723745412: : 1000it [17:26,  1.05s/it]    \n",
      "Epoch: 53, disc_loss:  0.17863046951984105, gen_loss: 0.5866092727058813: : 1000it [17:24,  1.04s/it]   \n",
      "Epoch: 54, disc_loss:  0.19734356729084984, gen_loss: 0.5894373731864126: : 1000it [17:23,  1.04s/it]   \n",
      "Epoch: 55, disc_loss:  0.17773198360936684, gen_loss: 0.5911520260415579: : 1000it [17:22,  1.04s/it]   \n",
      "Epoch: 56, disc_loss:  0.19258893831780083, gen_loss: 0.5913273196053087: : 1000it [17:20,  1.04s/it]   \n",
      "Epoch: 57, disc_loss:  0.21025675797148755, gen_loss: 0.5939666920194501: : 1000it [17:22,  1.04s/it]   \n",
      "Epoch: 58, disc_loss:  0.18415104925632478, gen_loss: 0.5952652306724013: : 1000it [17:22,  1.04s/it]   \n",
      "Epoch: 59, disc_loss:  0.18585375975098525, gen_loss: 0.5947856491594984: : 1000it [17:13,  1.03s/it]   \n",
      "Epoch: 60, disc_loss:  0.19858397866550245, gen_loss: 0.5889847195985024: : 1000it [17:14,  1.03s/it]  \n",
      "Epoch: 61, disc_loss:  0.1901694577461795, gen_loss: 0.5835104177709212: : 1000it [17:14,  1.03s/it]   \n",
      "Epoch: 62, disc_loss:  0.18179702265220776, gen_loss: 0.5919914445385598: : 1000it [17:15,  1.04s/it]   \n",
      "Epoch: 63, disc_loss:  0.19271818788428055, gen_loss: 0.5877188001965221: : 1000it [17:15,  1.04s/it]  \n",
      "Epoch: 64, disc_loss:  0.19376798783477983, gen_loss: 0.5841155511454532: : 1000it [17:13,  1.03s/it]   \n",
      "Epoch: 65, disc_loss:  0.18385463028623347, gen_loss: 0.5927274364965004: : 1000it [17:14,  1.03s/it]  \n",
      "Epoch: 66, disc_loss:  0.18512612431718592, gen_loss: 0.5863211392101488: : 1000it [17:16,  1.04s/it]   \n",
      "Epoch: 67, disc_loss:  0.181657343747323, gen_loss: 0.5877389546026264: : 1000it [17:15,  1.04s/it]     \n",
      "Epoch: 68, disc_loss:  0.1833034806450208, gen_loss: 0.5831699748415696: : 1000it [17:14,  1.03s/it]    \n",
      "Epoch: 69, disc_loss:  0.175602811323969, gen_loss: 0.5773545826108832: : 1000it [17:13,  1.03s/it]     \n",
      "Epoch: 70, disc_loss:  0.18204531086118597, gen_loss: 0.5872873845225887: : 1000it [17:15,  1.04s/it]  \n",
      "Epoch: 71, disc_loss:  0.18227415494751512, gen_loss: 0.5896116223042471: : 1000it [17:15,  1.04s/it]   \n",
      "Epoch: 72, disc_loss:  0.1787215944444924, gen_loss: 0.5927801659232692: : 1000it [17:13,  1.03s/it]    \n",
      "Epoch: 73, disc_loss:  0.18703432195542152, gen_loss: 0.5964238235197569: : 1000it [17:14,  1.03s/it]   \n",
      "Epoch: 74, disc_loss:  0.18638764174360978, gen_loss: 0.5958929153074298: : 1000it [17:16,  1.04s/it]   \n",
      "Epoch: 75, disc_loss:  0.18303909441881014, gen_loss: 0.5875328643489303: : 1000it [17:17,  1.04s/it]  \n",
      "Epoch: 76, disc_loss:  0.1761319915662732, gen_loss: 0.5864270337631828: : 1000it [17:18,  1.04s/it]    \n",
      "Epoch: 77, disc_loss:  0.17915553805075193, gen_loss: 0.5919208042349732: : 1000it [17:18,  1.04s/it]   \n",
      "Epoch: 78, disc_loss:  0.18306565303028677, gen_loss: 0.5863341155752801: : 1000it [17:15,  1.04s/it]   \n",
      "Epoch: 79, disc_loss:  0.187178556547876, gen_loss: 0.5912692704117088: : 1000it [17:15,  1.04s/it]     \n",
      "Epoch: 80, disc_loss:  0.19667043371158735, gen_loss: 0.5832987457722948: : 1000it [17:16,  1.04s/it]   \n",
      "Epoch: 81, disc_loss:  0.17982480583483712, gen_loss: 0.5864208078593538: : 1000it [17:16,  1.04s/it]   \n",
      "Epoch: 82, disc_loss:  0.17940220181879243, gen_loss: 0.5750609795252483: : 1000it [17:16,  1.04s/it]   \n",
      "Epoch: 83, disc_loss:  0.1847999776925957, gen_loss: 0.581591270942437: : 1000it [17:21,  1.04s/it]     \n",
      "Epoch: 84, disc_loss:  0.19507499656133484, gen_loss: 0.5883473753563144: : 1000it [17:19,  1.04s/it]   \n",
      "Epoch: 85, disc_loss:  0.18678577586224204, gen_loss: 0.5814169064111877: : 1000it [17:23,  1.04s/it]  \n"
     ]
    }
   ],
   "source": [
    "with open(\"logs.txt\") as f:\n",
    "    lines = f.read()\n",
    "    print(lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "images = []\n",
    "images_seg = []\n",
    "filenames = [filename for filename in os.listdir(\"./GanSEG/results/\") if \"seg\" not in filename]\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(os.path.join(\"./GanSEG/results/\", filename)))\n",
    "imageio.mimsave('./GanSEG/gan.gif', images, duration= 2)\n",
    "filenames_seg = [filename for filename in os.listdir(\"./GanSEG/results/\") if \"seg\" in filename]\n",
    "for filename in filenames_seg:\n",
    "    images_seg.append(imageio.imread(os.path.join(\"./GanSEG/results/\", filename)))\n",
    "imageio.mimsave('./GanSEG/gan_seg.gif', images_seg, duration= 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./GanSEG/gan.gif\" align=\"left\"/><img src=\"./GanSEG/gan_seg.gif\" align=\"left\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (keypoints_embedding): Embedding(8, 128)\n",
       "  (mask_spade_blocks): ModuleList(\n",
       "    (0): SPADE(\n",
       "      (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv6): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): SPADE(\n",
       "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): SPADE(\n",
       "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (3): SPADE(\n",
       "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv6): Conv2d(128, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (spade_blocks): ModuleList(\n",
       "    (0): SPADE(\n",
       "      (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): SPADE(\n",
       "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv6): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): SPADE(\n",
       "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (3): SPADE(\n",
       "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_std2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_mean2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (adain_blocks): ModuleList(\n",
       "    (0): AdaIN(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv_std): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (conv_mean): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): AdaIN(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv_std): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (conv_mean): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): AdaIN(\n",
       "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv_std): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv_mean): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (3): AdaIN(\n",
       "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (conv1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv_std): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv_mean): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (x_start): PositionalEmbedding(\n",
       "    (pe): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (mask_start): PositionalEmbedding(\n",
       "    (pe): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (bg_start): PositionalEmbedding(\n",
       "    (pe): Conv2d(2, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (rep_pad): ReplicationPad2d((10, 10, 10, 10))\n",
       "  (gen_keypoints_embedding_noise): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (gen_keypoints_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Linear(in_features=256, out_features=64, bias=True)\n",
       "  )\n",
       "  (gen_background_embedding): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_load = Params()\n",
    "\n",
    "device = 'cuda:0'\n",
    "device = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "generator = Generator(args).to(device)\n",
    "gen_checkpoint = torch.load(\"./GanSEG/checkpoints/best_model.model\",\n",
    "                            map_location=lambda storage, location: storage)\n",
    "generator.load_state_dict(gen_checkpoint['generator'])\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_input = {'input_noise{}'.format(noise_i): torch.randn(args.batch_size, *noise_shape).to(device)\n",
    "                    for noise_i, noise_shape in enumerate(generator.noise_shapes)}\n",
    "loaded_input['bg_trans'] = torch.rand(args.batch_size, 1, 2).to(device) * 2 - 1\n",
    "\n",
    "evaluate(generator, loaded_input, args, \"test\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./GanSEG/results/test.png\" align=\"left\"/><img src=\"./GanSEG/results/test_segmaps.png\" align=\"left\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challanges Faced\n",
    "### Model Architecture\n",
    "\n",
    "GANSeg model architecture basically consists of 2 different module: Generator and Discriminator. Discriminator is basically fully connected layer and it could be implemented according to what paper suggests. However, Generator module requires different submodules which is not explained in detail. SPADE and AdaIN sub modules can vbe given as example. Therefore, some assumptions were made by implementing this module. Also implementing forward function of the Generator module was pretty overwhelming because 3 important level which is explained in above were need to be implemented in here. Especially, \"Level 2: From Points to Masks\" part could not be impelemented properly because equations that paper suggests seem incomplete in terms of the matrix dimensions. In order to match matrix dimensions, some transpose operations were done. However, it is possible that these changes lead to some performance drops.\n",
    "\n",
    "### Dataset\n",
    "Dataset and Dataloader modules were implemented individually however, paper suggests that random input noises for generation and background and foreground of the images should be stored in a dictionary. Especially foreground and background information is not in default dataset folder. Therefore, these information are taken from the official implementation of the authors. Nevertheless, it is possible to download prepared dataset from drive link which is given in download_dataset.sh. Also it will be implemented in v2.\n",
    "\n",
    "### Gradient Penalty\n",
    "Gradient penalty is suggested in the different locations in the paper. Math part is the penalty seems incomplete in the paper and therefore, gradient penalty part could not be implemented in v2. \n",
    "\n",
    "### Hyperparemeter Tuning and Results\n",
    "Hyperparamaters were selected according to what paper suggests. These values lead to pretty successful performance(training loss). However, in results, some performance drop was observed in some class information such as blue region should not cover mouth and nose regions. Different hyperparameters values were used to optimize model but these do not affect performance remarkably. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
